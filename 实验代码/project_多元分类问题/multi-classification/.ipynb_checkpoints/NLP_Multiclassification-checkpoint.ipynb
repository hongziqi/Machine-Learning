{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\小柒\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import operator\n",
    "import re\n",
    "import math\n",
    "from functools import reduce\n",
    "import MC_MODEL as mc\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\n",
    "\t\"i\",          \"me\",         \"my\",         \"myself\",     \"we\",        \n",
    "\t\"our\",        \"ours\",       \"ourselves\",  \"you\",        \"your\",      \n",
    "\t\"yours\",      \"yourself\",   \"yourselves\", \"he\",         \"him\",       \n",
    "\t\"his\",        \"himself\",    \"she\",        \"her\",        \"hers\",      \n",
    "\t\"herself\",    \"it\",         \"its\",        \"itself\",     \"they\",      \n",
    "\t\"them\",       \"their\",      \"theirs\",     \"themselves\", \"what\",      \n",
    "\t\"which\",      \"who\",        \"whom\",       \"this\",       \"that\",      \n",
    "\t\"these\",      \"those\",      \"am\",         \"is\",         \"are\",       \n",
    "\t\"was\",        \"were\",       \"be\",         \"been\",       \"being\",     \n",
    "\t\"have\",       \"has\",        \"had\",        \"having\",     \"do\",        \n",
    "\t\"does\",       \"did\",        \"doing\",      \"would\",      \"should\",    \n",
    "\t\"could\",      \"ought\",      \"i'm\",        \"you're\",     \"he's\",      \n",
    "\t\"she's\",      \"it's\",       \"we're\",      \"they're\",    \"i've\",      \n",
    "\t\"you've\",     \"we've\",      \"they've\",    \"i'd\",        \"you'd\",     \n",
    "\t\"he'd\",       \"she'd\",      \"we'd\",       \"they'd\",     \"i'll\",      \n",
    "\t\"you'll\",     \"he'll\",      \"she'll\",     \"we'll\",      \"they'll\",   \n",
    "\t\"isn't\",      \"aren't\",     \"wasn't\",     \"weren't\",    \"hasn't\",    \n",
    "\t\"haven't\",    \"hadn't\",     \"doesn't\",    \"don't\",      \"didn't\",    \n",
    "\t\"won't\",      \"wouldn't\",   \"shan't\",     \"shouldn't\",  \"can't\",     \n",
    "\t\"cannot\",     \"couldn't\",   \"mustn't\",    \"let's\",      \"that's\",    \n",
    "\t\"who's\",      \"what's\",     \"here's\",     \"there's\",    \"when's\",    \n",
    "\t\"where's\",    \"why's\",      \"how's\",      \"a\",          \"an\",        \n",
    "\t\"the\",        \"and\",        \"but\",        \"if\",         \"or\",        \n",
    "\t\"because\",    \"as\",         \"until\",      \"while\",      \"of\",        \n",
    "\t\"at\",         \"by\",         \"for\",        \"with\",       \"about\",     \n",
    "\t\"against\",    \"between\",    \"into\",       \"through\",    \"during\",    \n",
    "\t\"before\",     \"after\",      \"above\",      \"below\",      \"to\",        \n",
    "\t\"from\",       \"up\",         \"down\",       \"in\",         \"out\",       \n",
    "\t\"on\",         \"off\",        \"over\",       \"under\",      \"again\",     \n",
    "\t\"further\",    \"then\",       \"once\",       \"here\",       \"there\",     \n",
    "\t\"when\",       \"where\",      \"why\",        \"how\",        \"all\",       \n",
    "\t\"any\",        \"both\",       \"each\",       \"few\",        \"more\",      \n",
    "\t\"most\",       \"other\",      \"some\",       \"such\",       \"no\",        \n",
    "\t\"nor\",        \"not\",        \"only\",       \"own\",        \"same\",      \n",
    "\t\"so\",         \"than\",       \"too\",        \"very\",       \"will\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "# \tstring = re.sub('- *[lr]rb *-', ' ', string)\n",
    "# \tstring = re.sub('(^| )http:[^ ]+', ' ', string)\n",
    "# \tstring = re.sub('(^| )[A-Za-z]( |$)', ' ', string)\n",
    "# \tstring = re.sub('(^| )\\'[^ ]+', ' ', string)\n",
    "# \tstring = re.sub('(^| )n\\'t ', ' ', string)\n",
    "# \tstring = re.sub('[^A-Za-z !?/]+', '', string)\n",
    "# \tstring = re.sub('(^| )[A-Za-z]( |$)', ' ', string)\n",
    "\tstring = re.sub('/', ' ', string)\n",
    "\treturn string\n",
    "def readData(fpath):\n",
    "    file = open(fpath, encoding='UTF-8')\n",
    "    label, sentences = [], []\n",
    "    for line in file:\n",
    "        l = line.split('\\t\\t')\n",
    "        label.append(l[0])\n",
    "        sen = list(map(clean, l[1].split('<sssss>')))\n",
    "        sen = list(map(lambda x:\n",
    "            list(filter(lambda y:y not in stopwords, x.split())), sen))\n",
    "        sen = list(filter(lambda x:x, sen))\n",
    "        sentences.append(sen if sen else [['qwerty']])\n",
    "    file.close()\n",
    "    return label, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('wordvec_6B_50d.txt', encoding='UTF-8')\n",
    "vec, wdvec_dict, order = [], {}, 0\n",
    "for line in file:\n",
    "\titem = line.strip().split()\n",
    "\tvec.append(list(map(float,item[1:])))\n",
    "\twdvec_dict[item[0]] = order\n",
    "\torder += 1\n",
    "file.close()\n",
    "def w2v(word):\n",
    "    if (word in wdvec_dict):\n",
    "        return np.array(vec[wdvec_dict[word]])\n",
    "    return np.zeros(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writefile(filename, res):\n",
    "    file = open(filename,'w')\n",
    "    for i in res:\n",
    "        file.write(i+'\\n')\n",
    "    file.close()\n",
    "    return\n",
    "def check_label(std, res):\n",
    "    return (np.array(res) == np.array(std)).sum() / len(std)\n",
    "def labelTonum(label):\n",
    "    L_y = list(map(lambda x:1 if x =='LOW' else 0, label))\n",
    "    L_y = np.array([L_y]).T\n",
    "    M_y = list(map(lambda x:1 if x =='MID' else 0, label))\n",
    "    M_y = np.array([M_y]).T\n",
    "    H_y = list(map(lambda x:1 if x =='HIG' else 0, label))\n",
    "    H_y = np.array([H_y]).T\n",
    "    return L_y, M_y, H_y\n",
    "def numTolabel(y):\n",
    "    name = ['LOW','MID','HIG']\n",
    "    return list(map(lambda x:L_name[x],y))\n",
    "def samplebalance(label, train, length):\n",
    "    l = list(map(lambda x:x[1] ,filter(lambda x: x[0]=='LOW',zip(label,range(len(slabel))))))\n",
    "    m = list(map(lambda x:x[1] ,filter(lambda x: x[0]=='MID',zip(label,range(len(slabel))))))\n",
    "    h = list(map(lambda x:x[1] ,filter(lambda x: x[0]=='HIG',zip(label,range(len(slabel))))))\n",
    "    index = l[:length] + m[:length] + h[:length] \n",
    "    label_ = list(map(lambda x: label[x],index))\n",
    "    train_ = np.array(list(map(lambda x: strain_x[x],index)))\n",
    "    return label_, train_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_Train_test(train_X, label, test_item):\n",
    "\tdist = train_X - test_item\n",
    "\tdist = np.sum(dist * dist, axis=1)\n",
    "\titem = zip(dist, label)\n",
    "\treturn sorted(item, key=lambda x:x[0], reverse=True)\n",
    "\n",
    "def evaluate(TP, FN, TN, FP):\n",
    "\tAccuracy = (TP + TN) / np.float64(TP + FP + TN + FN)\n",
    "\treturn Accuracy\n",
    "def check(res, std):\n",
    "\tcount = [0]*4\n",
    "\tfor r, s in zip(res, std):\n",
    "\t\twrong = int(r != s)\n",
    "\t\tneg = int(s != 1)\n",
    "\t\tcount[neg * 2 + wrong] += 1\n",
    "\tprint(evaluate(*count))\n",
    "\treturn \n",
    "def sigmoid(X):\n",
    "\treturn 1 / (1 + np.exp(-X))\n",
    "class LogicalRegression(object):\n",
    "\t\"\"\"docstring for LogicalRegression\"\"\"\n",
    "\tdef __init__(self, train_X, train_y, weights=None, times=1000, alpha=0.01, c=2, style='batch'):\n",
    "\t\tself.train_X = train_X\n",
    "\t\tself.train_y = train_y\n",
    "\t\tself.times = times\n",
    "\t\tself.alpha = alpha\n",
    "\t\tself.c = c\n",
    "\t\tgradient = []\n",
    "\t\tif weights == None:\n",
    "\t\t\tweights = np.zeros((1,train_X.shape[1])) # 1*M\n",
    "\t\t# alpha = 0.000001\n",
    "\t\tepsilon = 1e-6\n",
    "\t\tself.w0 = weights.T # M*1\n",
    "\t\tp = sigmoid(np.dot(train_X, self.w0)) # N*M X M*1 = N*1\n",
    "\t\tJ0 = self.logistic_cost(p, train_y)\n",
    "\t\tK0 = K00 = J0\n",
    "\t\tfor t in range(times):\n",
    "\t\t\tp_y = p - train_y\n",
    "\t\t\tw = self.w0 - alpha * np.dot(train_X.T, p_y) \n",
    "\t\t\t# print(np.dot(train_X.transpose(), p_y))\n",
    "\t\t\tgradient.append(np.sum(-np.dot(train_X.T, p_y)))\n",
    "\t\t\t# w = self.w0 - alpha * np.dot(train_X.transpose(), p_y) / len(p) - lambda_ * self.w0 / len(p) #40*1\n",
    "\t\t\tp = sigmoid(np.dot(train_X, w))\n",
    "\t\t\tJ = self.logistic_cost(p, train_y)\n",
    "\t\t\tif t > 7 and K00 == J:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tK00 = K0\n",
    "\t\t\tK0 = J\n",
    "\t\t\tif J < J0: # not convergent\n",
    "\t\t\t\tJ0 = J\n",
    "\t\t\t\tself.w0 = w\n",
    "\t\t\telse:\n",
    "\t\t\t\talpha /= c\n",
    "\t\t\t\tif np.linalg.norm(w - self.w0) < epsilon * np.linalg.norm(self.w0):\n",
    "\t\t\t\t\t# print('times: ', t)\n",
    "\t\t\t\t\tbreak\n",
    "\t\treturn # M*1\n",
    "\tdef LR(self, train_X, train_y,weights=None):\n",
    "\t\tgradient = []\n",
    "\t\tif weights == None:\n",
    "\t\t\tweights = np.zeros((1,train_X.shape[1])) # 1*M\n",
    "\t\t# alpha = 0.000001\n",
    "\t\tepsilon = 1e-3\n",
    "\t\tw0 = weights.T # M*1\n",
    "\t\t# print(w0)\n",
    "\t\tp = sigmoid(np.dot(train_X, w0)) # N*M X M*1 = N*1\n",
    "\t\tJ0 = self.logistic_cost(p, train_y)\n",
    "\t\tK0 = K00 = J0\n",
    "\t\tfor t in range(self.times):\n",
    "\t\t\tp_y = p - train_y\n",
    "\t\t\tw = w0 - self.alpha * np.dot(train_X.T, p_y) \n",
    "\t\t\t# print(np.dot(train_X.transpose(), p_y))\n",
    "\t\t\tgradient.append(np.sum(-np.dot(train_X.T, p_y)))\n",
    "\t\t\t# w = w0 - alpha * np.dot(train_X.transpose(), p_y) / len(p) - lambda_ * w0 / len(p) #40*1\n",
    "\t\t\tp = sigmoid(np.dot(train_X, w))\n",
    "\t\t\tJ = self.logistic_cost(p, train_y)\n",
    "\t\t\tif t > 7 and K00 == J:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tK00 = K0\n",
    "\t\t\tK0 = J\n",
    "\t\t\tif J < J0: # not convergent\n",
    "\t\t\t\tJ0 = J\n",
    "\t\t\t\tw0 = w\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.alpha /= self.c\n",
    "\t\t\t\tif np.linalg.norm(w - w0) < epsilon * np.linalg.norm(w0):\n",
    "\t\t\t\t\t# print('times: ', t)\n",
    "\t\t\t\t\tbreak\n",
    "\t\treturn w0 # M*1\n",
    "\tdef logistic_cost(self, p, train_y):\n",
    "\t\t_p = copy.deepcopy(p)\n",
    "\t\t_p[train_y == 0] = 1 - _p[train_y == 0]\n",
    "\t\treturn -sum(np.log(_p+0.000001)) \n",
    "\tdef predict(self, test_X):\n",
    "\t\treturn  sigmoid(np.dot(test_X, self.w0))\n",
    "\tdef groupValidation(self, k=10):\n",
    "\t\tn = self.train_X.shape[0]\n",
    "\t\tm = math.ceil(n/k)\n",
    "\t\tn_id = list(range(n))\n",
    "\t\trandom.shuffle(n_id)\n",
    "\t\tL = []\n",
    "\t\tL += list(map(lambda x:n_id[x:x+m] ,map(lambda i:i*m, range(10))))\n",
    "\t\treturn L\n",
    "\tdef crossValidation(self):\n",
    "\t\tL = self.groupValidation()\n",
    "\t\tfor i in range(len(L)):\n",
    "\t\t\tcp_L = copy.deepcopy(L)\n",
    "\t\t\tvali_id = cp_L.pop(i)\n",
    "\t\t\tvali_X, vali_y = self.train_X[vali_id], self.train_y[vali_id]\n",
    "\t\t\ttra_x, tra_y = self.train_X[reduce(operator.add,cp_L)], self.train_y[reduce(operator.add,cp_L)]\n",
    "\t\t\tweights = self.LR(tra_x, tra_y)\n",
    "\t\t\tres = np.dot(vali_X, weights)\n",
    "\t\t\tres = list(map(lambda x: 1 if x > 0 else 0, res))\n",
    "\t\t\tcheck(res, vali_y)\n",
    "\t\treturn\n",
    "    \n",
    "class Mini_Batch_BilayerRModel(object):\n",
    "\tdef __init__(self, train_X, label, hwidth=14, times=10000, owidth=3):\n",
    "\t\tm, xdim = train_X.shape\n",
    "\t\talpha, eps = 1e-5, 1e-10\n",
    "\t\tnr = np.random.rand\n",
    "\t\tself.w = [nr(xdim, hwidth), nr(hwidth, owidth)]\n",
    "\t\tself.b = [nr(1, hwidth), nr(1, owidth)]\n",
    "# \t\tself.losses = {'train':[], 'validation':[]}\n",
    "\t\twc, bc = copy.deepcopy(self.w), copy.deepcopy(self.b)\n",
    "\t\ttrain_y = self.matrix_train(label) # (m, 3)\n",
    "\t\tJ0 = self.logloss(train_X, train_y)\n",
    "# \t\tself.check(train_X, label)\n",
    "\t\t# k = math.ceil(0.777 * m)\n",
    "\t\tfor t in range(times):\n",
    "\t\t\tfor i in random.sample(range(m), m):\t\t\n",
    "\t\t\t\to1 = sigmoid(np.dot(train_X[[i]], self.w[0]) + self.b[0]) # (1, hwidth)\n",
    "\t\t\t\to2 = sigmoid(np.dot(o1, self.w[1]) + self.b[1]) # (1, 3)\n",
    "\t\t\t\ttmp = np.zeros((1,3))\n",
    "\t\t\t\tneg = train_y[i].reshape(o2.shape) == 0\n",
    "\t\t\t\ttmp[neg] = 1/(1-o2[neg])\n",
    "\t\t\t\ttmp[~neg] = -1/o2[~neg]\n",
    "\t\t\t\tg = tmp*o2*(1-o2)  # (1, 3)\n",
    "\t\t\t\te = o1 * (1 - o1) * np.dot(g, self.w[1].T) # (1, hwidth)\n",
    "\t\t\t\tself.w[1] -= alpha * g * o1.T # (hwidth, 3)\n",
    "\t\t\t\tself.b[1] -= alpha * g\n",
    "\t\t\t\tself.w[0] -= alpha * np.dot(train_X[[i]].T, e) #(50, hwidth)\n",
    "\t\t\t\tself.b[0] -= alpha * e\t\t\n",
    "\t\t\tJ = self.logloss(train_X, train_y)\n",
    "\t\t\tprint('times:', t, ' J:', J, ' J0:', J0)\n",
    "\t\t\tif t == 99:\n",
    "\t\t\t\tself.check(train_X, label)\n",
    "\t\t\tif J < J0:\n",
    "\t\t\t\twc, bc = copy.deepcopy(self.w), copy.deepcopy(self.b)\n",
    "\t\t\t\tif (J0 - J) / J < eps:\n",
    "\t\t\t\t\tprint('times:', t)\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\tJ0 = J\n",
    "\t\t\telse:\n",
    "\t\t\t\t# if np.linalg.norm(self.w[0] - wc[0]) < eps * np.linalg.norm(wc[0]):\n",
    "\t\t\t\t# \tprint('times:', t)\n",
    "\t\t\t\t# \tbreak\n",
    "\t\t\t\talpha /= 2\n",
    "\t\t\t\tself.w, self.b = wc, bc\n",
    "\t\tpass\n",
    "\tdef loss(self):\n",
    "\t\treturn self.losses\n",
    "\tdef predict(self, test_X):\n",
    "\t\tm = test_X.shape[0]\n",
    "\t\tb = sigmoid(np.dot(test_X, self.w[0]) + self.b[0])  # (m, hwidth)\n",
    "\t\tp = sigmoid(np.dot(b, self.w[1]) + self.b[1])       # (m, 3)\n",
    "\t\treturn p\n",
    "\tdef matrix_train(self, train_y):\n",
    "\t\tl = np.array([list(map(lambda x: 1 if x == 'LOW' else 0, train_y))])\n",
    "\t\tm = np.array([list(map(lambda x: 1 if x == 'MID' else 0, train_y))])\n",
    "\t\th = np.array([list(map(lambda x: 1 if x == 'HIG' else 0, train_y))])\n",
    "\t\treturn np.concatenate((l,m,h),axis=0).T # (m, 3)\n",
    "\tdef logloss(self, train_X, train_y):\n",
    "\t\tp = self.predict(train_X)\n",
    "\t\ttmp = np.zeros(p.shape)\n",
    "\t\tneg = train_y == 0\n",
    "\t\ttmp[neg] = np.log(1-p[neg])\n",
    "\t\ttmp[~neg] = np.log(p[~neg])\n",
    "\t\treturn -sum(sum(tmp))\n",
    "\t\t# return -sum(sum(train_y*np.log(p) + (1-train_y)*np.log(1-p)))\n",
    "\tdef check(self, train_X, label):\n",
    "\t\tp = self.predict(train_X)\n",
    "\t\t_res = p.argmax(axis = 1)\n",
    "\t\tL_name = ['LOW', 'MID', 'HIG']\n",
    "\t\tres = list(map(lambda x: L_name[x], _res))\n",
    "\t\tprint(res)\n",
    "\t\tac = (np.array(res) == np.array(label)).sum() / len(label)\n",
    "\t\tprint(ac)\n",
    "\t\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, train_sen = readData('MulLabelTrain.ss')\n",
    "_, test_sen = readData('MulLabelTest.ss')\n",
    "vlabel, svalid_sen = readData('SmallValid.ss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "for ts in train_sen:\n",
    "    v = [np.array(list(map(w2v,s))).mean(axis=0) for s in ts]\n",
    "    v = np.array(v).mean(axis=0)\n",
    "    train_x.append(v)\n",
    "train_x = np.array(train_x)\n",
    "tx0 = np.ones((train_x.shape[0], 1))\n",
    "train_x = np.concatenate((tx0, train_x), axis=1)\n",
    "\n",
    "test_x = []\n",
    "for ts in test_sen:\n",
    "    v = [np.array(list(map(w2v,s))).mean(axis=0) for s in ts]\n",
    "    v = np.array(v).mean(axis=0)\n",
    "    test_x.append(v)\n",
    "test_x = np.array(test_x)\n",
    "tx0 = np.ones((test_x.shape[0], 1))\n",
    "test_x = np.concatenate((tx0, test_x), axis=1)\n",
    "valid_x = []\n",
    "for ts in valid_sen:\n",
    "    v = np.array(list(map(w2v,ts))).mean(axis=0)\n",
    "    valid_x.append(v)\n",
    "valid_x = np.array(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('train.txt', 'w')\n",
    "# i = 0\n",
    "# for ts,l in zip(train_sen, label):\n",
    "#     v = [np.array(list(map(w2v,s))).mean(axis=0) for s in ts]\n",
    "#     v = np.array(v).mean(axis=0)\n",
    "#     i += 1\n",
    "#     v_str = ','.join(list(map(str,v)))\n",
    "#     file.write(v_str + ',' + l + '\\n')\n",
    "# file.close()\n",
    "\n",
    "# file = open('test.txt', 'w')\n",
    "# for ts in test_sen:\n",
    "#     v = [np.array(list(map(w2v,s))).mean(axis=0) for s in ts]\n",
    "#     v = np.array(v).mean(axis=0)\n",
    "#     v_str = ','.join(list(map(str,v)))\n",
    "#     file.write(v_str + '\\n')\n",
    "# file.close()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################KNN Validtion##########################\n",
    "slabel, strain_x = samplebalance(label, train_x, 1000)\n",
    "K = 2\n",
    "KNN_pred = []\n",
    "for item in valid_x:\n",
    "    NN = dist_Train_test(strain_x, slabel, item)\n",
    "    KNN_pred += [Counter(list(map(lambda x:x[1],NN[:K]))).most_common(1)[0][0]]\n",
    "check_label(KNN_pred, vlabel)\n",
    "####################################KNN Validtion##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################LR Validtion############################\n",
    "x0 = np.ones((strain_x.shape[0],1))\n",
    "stx = np.concatenate((x0, strain_x), axis=1)\n",
    "x0 = np.ones((valid_x.shape[0], 1))\n",
    "vx = np.concatenate((x0, valid_x), axis=1)\n",
    "\n",
    "L_y, M_y, H_y = labelTonum(slabel)\n",
    "model_L = LogicalRegression(stx, L_y)\n",
    "model_M = LogicalRegression(stx, M_y)\n",
    "model_H = LogicalRegression(stx, H_y)\n",
    "pred_L = model_L.predict(vx)\n",
    "pred_M = model_M.predict(vx)\n",
    "pred_H = model_H.predict(vx)\n",
    "pred = np.concatenate((pred_L, pred_M, pred_H), axis=1)\n",
    "_res = pred.argmax(axis=1)\n",
    "res = numTolabel(_res)\n",
    "check_label(res, vlabel)\n",
    "###################################LR Validtion############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################BPNN###################################\n",
    "BPNN = Mini_Batch_BilayerRModel(stx,slabel,hwidth=30, times=100)\n",
    "res = BPMM.predict(vx).argmsx(axis=1)\n",
    "res = numTolabel(_res)\n",
    "check_label(res, vlabel)\n",
    "###################################BPNN###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5b65df78555e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m################################################ XGB #####################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mD_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'LOW'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'MID'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'HIG'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnum_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mD_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mxg_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mxg_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "################################################ XGB #####################################\n",
    "D_name = {'LOW':0, 'MID':1, 'HIG':2}\n",
    "num_y = np.array([list(map(lambda x: D_name[x], label))]).T\n",
    "xg_train = xgb.DMatrix(train_x, num_y)\n",
    "xg_test = xgb.DMatrix(test_x)\n",
    "xgb_params = {\n",
    "    'eta':0.05,\n",
    "    'max_depth':6,\n",
    "    'silent':1,\n",
    "    'num_class':3,\n",
    "    'objective':'multi:softprob',\n",
    "    'subsample':0.7,\n",
    "    'colsample_bytree':1.0\n",
    "}\n",
    "model = xgb.train(xgb_params, xg_train, num_boost_round= 1000)\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\小柒\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "###################################KNN test################################\n",
    "newlabel, newtrain = samplebalance(label, train_x, 17000)\n",
    "K = 2\n",
    "KNN_pred = []\n",
    "for item in test_x:\n",
    "    NN = dist_Train_test(newtrain, newlabel, item)\n",
    "    KNN_pred += [Counter(list(map(lambda x:x[1],NN[:K]))).most_common(1)[0][0]]\n",
    "writefile('KNN.csv', KNN_pred)\n",
    "###################################KNN test################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62522, 51)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################LR test################################\n",
    "x0 = np.ones((train_x.shape[0],1))\n",
    "trainx = np.concatenate((x0, train_x), axis=1)\n",
    "x0 = np.ones((test_x.shape[0], 1))\n",
    "testx = np.concatenate((x0, test_x), axis=1)\n",
    "\n",
    "L_y, M_y, H_y = labelTonum(label)\n",
    "model_L = LogicalRegression(trainx, L_y)\n",
    "model_M = LogicalRegression(trainx, M_y)\n",
    "model_H = LogicalRegression(trainx, H_y)\n",
    "pred_L = model_L.predict(testx)\n",
    "pred_M = model_M.predict(testx)\n",
    "pred_H = model_H.predict(testx)\n",
    "pred = np.concatenate((pred_L, pred_M, pred_H), axis=1)\n",
    "_res = pred.argmax(axis=1)\n",
    "LR_pred = numTolabel(_res)\n",
    "writefile('LR.csv', LR_pred)\n",
    "###################################LR test################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(range(len(pred.argmax(axis=1))), pred.argmax(axis=1))\n",
    "# plt.show()\n",
    "# plt.hist( pred.argmax(axis=1))\n",
    "# plt.show()\n",
    "# pred_L0 = model_L.predict(train_x[:500])\n",
    "# pred_M0 = model_M.predict(train_x[:500])\n",
    "# pred_H0 = model_H.predict(train_x[:500])\n",
    "# pred0 = np.concatenate((pred_L0, pred_M0, pred_H0), axis=1)\n",
    "# ac = (np.array(pred0) == np.array(label[:500])).sum()/ len(label)\n",
    "# print(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################LR validation###########################\n",
    "strain_x = []\n",
    "for ts in strain_sen:\n",
    "    v = [np.array(list(map(w2v,s))).mean(axis=0) for s in ts]\n",
    "    v = np.array(v).mean(axis=0)\n",
    "    strain_x.append(v)\n",
    "strain_x = np.array(strain_x)\n",
    "x0 = np.ones((strain_x.shape[0], 1))\n",
    "strain_x = np.concatenate((x0, strain_x), axis=1)\n",
    "svalid_x = []\n",
    "for ts in svalid_sen:\n",
    "    v = [np.array(list(map(w2v,s))).mean(axis=0) for s in ts]\n",
    "    v = np.array(v).mean(axis=0)\n",
    "    svalid_x.append(v)\n",
    "svalid_x = np.array(svalid_x)\n",
    "x0 = np.ones((svalid_x.shape[0], 1))\n",
    "svalid_x = np.concatenate((x0, svalid_x), axis=1)\n",
    "\n",
    "sL_y = list(map(lambda x:1 if x =='LOW' else 0, slabel))\n",
    "sL_y = np.array([sL_y]).T\n",
    "sM_y = list(map(lambda x:1 if x =='MID' else 0, slabel))\n",
    "sM_y = np.array([sM_y]).T\n",
    "sH_y = list(map(lambda x:1 if x =='HIG' else 0, slabel))\n",
    "sH_y = np.array([sH_y]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "smodel_L = LogicalRegression(strain_x, sL_y)\n",
    "#pred_L[:20]\n",
    "smodel_M = LogicalRegression(strain_x, sM_y)\n",
    "smodel_H = LogicalRegression(strain_x, sH_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.564\n"
     ]
    }
   ],
   "source": [
    "spred_L = smodel_L.predict(svalid_x)\n",
    "spred_M = smodel_M.predict(svalid_x)\n",
    "spred_H = smodel_H.predict(svalid_x)\n",
    "spred = np.concatenate((spred_L, spred_M, spred_H), axis=1)\n",
    "spred = spred.argmax(axis=1)\n",
    "L_name = ['LOW','MID','HIG']\n",
    "spred = list(map(lambda x:L_name[x], spred))\n",
    "ac = (np.array(spred) == np.array(v_label)).sum()/ len(v_label)\n",
    "print(ac)\n",
    "###############################LR validation###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.264\n"
     ]
    }
   ],
   "source": [
    "# K = 5\n",
    "# KNN_pred = []\n",
    "# for item in svalid_x:\n",
    "#     NN = dist_Train_test(strain_x, slabel, item)\n",
    "#     KNN_pred += [Counter(list(map(lambda x:x[1],NN[:K]))).most_common(1)[0][0]]\n",
    "# ac = (np.array(v_label) == np.array(KNN_pred)).sum() / len(v_label)\n",
    "# print(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "# K = 2\n",
    "# KNN_pred = []\n",
    "# for item in test_x:\n",
    "#     NN = dist_Train_test(train_x, label, item)\n",
    "#     KNN_pred += [Counter(list(map(lambda x:x[1],NN[:K]))).most_common(1)[0][0]]\n",
    "# file = open('KNNsen.csv','w')\n",
    "# for i in KNN_pred:\n",
    "#     file.write(i + '\\n')\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times: 0  J: 56658.7850161  J0: 58268.9679897\n",
      "times: 1  J: 55059.3858268  J0: 56658.7850161\n",
      "times: 2  J: 53470.8265005  J0: 55059.3858268\n",
      "times: 3  J: 51893.1562248  J0: 53470.8265005\n",
      "times: 4  J: 50326.481973  J0: 51893.1562248\n",
      "times: 5  J: 48770.7460779  J0: 50326.481973\n",
      "times: 6  J: 47226.0119621  J0: 48770.7460779\n",
      "times: 7  J: 45692.3423546  J0: 47226.0119621\n",
      "times: 8  J: 44169.6941955  J0: 45692.3423546\n",
      "times: 9  J: 42658.0587309  J0: 44169.6941955\n",
      "times: 10  J: 41157.4263162  J0: 42658.0587309\n",
      "times: 11  J: 39667.82218  J0: 41157.4263162\n",
      "times: 12  J: 38189.1844431  J0: 39667.82218\n",
      "times: 13  J: 36721.5700104  J0: 38189.1844431\n",
      "times: 14  J: 35264.9216537  J0: 36721.5700104\n",
      "times: 15  J: 33819.3463763  J0: 35264.9216537\n",
      "times: 16  J: 32384.8863376  J0: 33819.3463763\n",
      "times: 17  J: 30961.6330932  J0: 32384.8863376\n",
      "times: 18  J: 29549.8454473  J0: 30961.6330932\n",
      "times: 19  J: 28149.7451919  J0: 29549.8454473\n",
      "times: 20  J: 26761.8667999  J0: 28149.7451919\n",
      "times: 21  J: 25386.8909225  J0: 26761.8667999\n",
      "times: 22  J: 24025.713354  J0: 25386.8909225\n",
      "times: 23  J: 22679.7046284  J0: 24025.713354\n",
      "times: 24  J: 21350.7330502  J0: 22679.7046284\n",
      "times: 25  J: 20041.3213914  J0: 21350.7330502\n",
      "times: 26  J: 18754.6024046  J0: 20041.3213914\n",
      "times: 27  J: 17494.8075906  J0: 18754.6024046\n",
      "times: 28  J: 16267.1649275  J0: 17494.8075906\n",
      "times: 29  J: 15077.7946219  J0: 16267.1649275\n",
      "times: 30  J: 13933.3697086  J0: 15077.7946219\n",
      "times: 31  J: 12841.1546049  J0: 13933.3697086\n",
      "times: 32  J: 11807.4013916  J0: 12841.1546049\n",
      "times: 33  J: 10838.6311785  J0: 11807.4013916\n",
      "times: 34  J: 9939.02402108  J0: 10838.6311785\n",
      "times: 35  J: 9111.94057029  J0: 9939.02402108\n",
      "times: 36  J: 8357.77168058  J0: 9111.94057029\n",
      "times: 37  J: 7677.31929702  J0: 8357.77168058\n",
      "times: 38  J: 7069.22870367  J0: 7677.31929702\n",
      "times: 39  J: 6531.81129223  J0: 7069.22870367\n",
      "times: 40  J: 6062.7126716  J0: 6531.81129223\n",
      "times: 41  J: 5658.49693901  J0: 6062.7126716\n",
      "times: 42  J: 5315.04294928  J0: 5658.49693901\n",
      "times: 43  J: 5027.15843682  J0: 5315.04294928\n",
      "times: 44  J: 4788.87649419  J0: 5027.15843682\n",
      "times: 45  J: 4593.85367375  J0: 4788.87649419\n",
      "times: 46  J: 4435.67447522  J0: 4593.85367375\n",
      "times: 47  J: 4308.281365  J0: 4435.67447522\n",
      "times: 48  J: 4206.28586389  J0: 4308.281365\n",
      "times: 49  J: 4124.90307185  J0: 4206.28586389\n",
      "times: 50  J: 4060.12316721  J0: 4124.90307185\n",
      "times: 51  J: 4008.42655012  J0: 4060.12316721\n",
      "times: 52  J: 3967.31159725  J0: 4008.42655012\n",
      "times: 53  J: 3934.55979658  J0: 3967.31159725\n",
      "times: 54  J: 3908.45243419  J0: 3934.55979658\n",
      "times: 55  J: 3887.60197566  J0: 3908.45243419\n",
      "times: 56  J: 3870.89716775  J0: 3887.60197566\n",
      "times: 57  J: 3857.53179061  J0: 3870.89716775\n",
      "times: 58  J: 3846.80944436  J0: 3857.53179061\n",
      "times: 59  J: 3838.17055303  J0: 3846.80944436\n",
      "times: 60  J: 3831.23724125  J0: 3838.17055303\n",
      "times: 61  J: 3825.6155951  J0: 3831.23724125\n",
      "times: 62  J: 3821.08016651  J0: 3825.6155951\n",
      "times: 63  J: 3817.38688776  J0: 3821.08016651\n",
      "times: 64  J: 3814.40237222  J0: 3817.38688776\n",
      "times: 65  J: 3811.95741411  J0: 3814.40237222\n",
      "times: 66  J: 3809.97539891  J0: 3811.95741411\n",
      "times: 67  J: 3808.35280836  J0: 3809.97539891\n",
      "times: 68  J: 3807.03306991  J0: 3808.35280836\n",
      "times: 69  J: 3805.95259403  J0: 3807.03306991\n",
      "times: 70  J: 3805.06132118  J0: 3805.95259403\n",
      "times: 71  J: 3804.31827899  J0: 3805.06132118\n",
      "times: 72  J: 3803.70314058  J0: 3804.31827899\n",
      "times: 73  J: 3803.19311544  J0: 3803.70314058\n",
      "times: 74  J: 3802.76247506  J0: 3803.19311544\n",
      "times: 75  J: 3802.40514414  J0: 3802.76247506\n",
      "times: 76  J: 3802.10293092  J0: 3802.40514414\n",
      "times: 77  J: 3801.84844018  J0: 3802.10293092\n",
      "times: 78  J: 3801.62768481  J0: 3801.84844018\n",
      "times: 79  J: 3801.4388525  J0: 3801.62768481\n",
      "times: 80  J: 3801.27346489  J0: 3801.4388525\n",
      "times: 81  J: 3801.13153  J0: 3801.27346489\n",
      "times: 82  J: 3801.00600366  J0: 3801.13153\n",
      "times: 83  J: 3800.89355599  J0: 3801.00600366\n",
      "times: 84  J: 3800.79045937  J0: 3800.89355599\n",
      "times: 85  J: 3800.6967405  J0: 3800.79045937\n",
      "times: 86  J: 3800.61111969  J0: 3800.6967405\n",
      "times: 87  J: 3800.53179593  J0: 3800.61111969\n",
      "times: 88  J: 3800.45713755  J0: 3800.53179593\n",
      "times: 89  J: 3800.38671618  J0: 3800.45713755\n",
      "times: 90  J: 3800.31993599  J0: 3800.38671618\n",
      "times: 91  J: 3800.25542177  J0: 3800.31993599\n",
      "times: 92  J: 3800.19307567  J0: 3800.25542177\n",
      "times: 93  J: 3800.13262004  J0: 3800.19307567\n",
      "times: 94  J: 3800.07428292  J0: 3800.13262004\n",
      "times: 95  J: 3800.01712008  J0: 3800.07428292\n",
      "times: 96  J: 3799.96048627  J0: 3800.01712008\n",
      "times: 97  J: 3799.90574803  J0: 3799.96048627\n",
      "times: 98  J: 3799.85168243  J0: 3799.90574803\n",
      "times: 99  J: 3799.79803961  J0: 3799.85168243\n",
      "['MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'LOW', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'LOW', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'MID', 'LOW']\n",
      "0.4095\n"
     ]
    }
   ],
   "source": [
    "###BPNN\n",
    "BPNN = Mini_Batch_BilayerRModel(strain_x,slabel,hwidth=35, times=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.05421906,  0.41508464,  0.30205989,  0.87784964,  0.08557019,\n",
       "          0.0378227 ,  0.60274701,  0.89351973,  0.53737638,  0.77399715,\n",
       "          0.33047274,  0.37381294,  0.42897252,  0.23650301]]),\n",
       " array([[ 0.77372919,  0.37010699,  0.26151124]])]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BPNN.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BPNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-20af6ebe3704>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mBPNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'BPNN' is not defined"
     ]
    }
   ],
   "source": [
    "BPNN.w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
