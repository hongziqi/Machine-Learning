# 机器学习

实现了机器学习中基本的算法。（Python）。这里简单介绍了实验的算法以及针对解决的问题。详细代码及原理可以参考实验代码以及报告。

##Lab1——文件读取

* 文件读写以及字符串分割：

参考链接：

Python：<https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431917715991ef1ebc19d15a4afdace1169a464eecc2000>

[http://blog.sina.com.cn/s/](http://blog.sina.com.cn/s/blog_81e6c30b01019wro.html)[blog_81e6c30b01019wro.html](http://blog.sina.com.cn/s/blog_81e6c30b01019wro.html)

将数据集“semeval”的数据表示成 One-hot矩阵，TF矩阵，TF-IDF 矩阵。

（三种矩阵定义可参考实验要求下的lab1 文件读取.pptx）

## Lab2——K近邻与朴素贝叶斯（分类和回归）

####K近邻 

用KNN算法处理分类问题，输出为类标签。处理回归问题输出某一类的概率。一般K近邻采用的距离公式为曼哈顿距离、欧式距离、余弦相似度。

——分类中通过前 K 个之后计算得到出现次数最多的情感类属，将其作为验证集样本的预测情感。

——回归中通过由前 K 个样本的情感归属概率加权求和，归一化后得到测试样本的预测情感归属概率（权值取测试样本与训练样本之间距离的倒数）。

——通过验证集对参数K以及不同的距离公式进行调优，考虑删去部分没有意义的虚词。

####朴素贝叶斯 

朴素贝叶斯的思想则是认为给定样本每个特征与其他特征都不相关。

——分类问题下有伯努利模型和多项式模型，两者的计算粒度是不同的，伯努利模型基于文件，而多项式基于特征词。

——回归问题和分类最大的不同就是情感属性为一个概率，分类问题中可以将相同属性的文本结合，而回归中只能通过情感概率乘上测试单词出现的概率得到这个文本以及测试单词下的情感指数，然后将每个文本求和得到测试单词的情感指数。

——采用拉普拉斯平滑，测试样本中如果出现了训练集中从来没有出现过的特征词，根据公式计算会导致概率为 0，这是不合理的（一方面考虑到没有在训练集出现的单词并不意味着它就不是这一类别下的特征词，另一方面概率的连乘会导致原本类属那个情感下的概率一下子变为 0，这也是不合理的，不能由于一个没有出现的特征词就否定了前面特征词的概率）。

$ P_i =\frac{x_i+\alpha}{N+M}  $ 

其中$x_𝑗$ 表示特征词出现的个数，N 表示总的特征词（包含重复出现的特征词），M 表示总的不重复的特征词。

分享一篇关于朴素贝叶斯的博客：https://www.cnblogs.com/lesliexong/p/6907642.html

### Lab3——感知机学习算法（PLA）  

原始的感知算法使处理二分类问题的，基于线性可分的数据集，可以找出一个超平面对样本进行分类。存在着一个超平面对 n 维的空间进行切割实现对样本的二分类。那么就可以定义一个 n+1 维的特征
向量 x = {1，x1，x2，x3，....，xn}和一个 n+1 维的权重向量 w = {w0，w1，w2，....，wn}
来表示这个超平面，超平面的表达公式为：

$$w0 + w1 ∗ x1 + w2 ∗ x2 + ⋯+ wn ∗ xn = 0$$

超平面将 n 维的样本数据划分为两类，一类为正样本，另一类为负样本。定义一个符号函数 $sign（x）=  return x > 0 ? +1 : -1$，预测的 y 就可以表示为：

$$y = sign(\sum_{i=0}^{n}w_i*x_i)$$

原始的感知学习机是随机定义权重向量，遍历训练集，一旦 𝑧̂ ! = 𝑧 那么就根据$w_{i+1} = w_i + y*x$ 进行更新权重，直到所有的预测都正确。由于现实生活中真正线性可分的模型是极少的，所以感知学习机需要设置通过设置阈值或者迭代次数的方法停止感知。

####口袋算法 

口袋算法：基于原始感知机上一种改进。由于原始感知机得到的是最后的 w，对于非线性模型来说，更改后的 w 不一定会比原来的 w 好，所以需要在更改 w 的时候需要对比较好的进行保存，遍历结束后就可以得到遍历过程中最好的 w。

####梯度下降 

采用梯度下降，引入损失函数，对损失函数求导，就得到下降损失的方向，只要w沿着损失方向进行更新，就可以另损失函数最小，意味着拟合度越高，模型越好。在线性回归中，损失函数通常为样本标准值与预测值的差取平方。

$ J(w) = 1/2 \sum_{i=1}^{M}(h_w(x^i) - y^i)^2 ， w_j = w_j - \alpha \frac{\partial }{\partial w_j}J(w) $  

梯度算法又分为随机梯度下降和批量梯度下降算法。随机梯度下降算法和批量梯度算法只有一点不同，批量算法迭代更新 w 值的使用了整个样本值，算法复杂度为 O（Mn）。而随机梯度下降每读取一条样本就对 w 进行更新，算法复杂度为 O（n）。注意，对于大数据，可能读取一部分数据就会使函数收敛，所有导致了可能不能收敛于最小值，而是在最小值附近震荡。

随机梯度下降更新公式：

$ w_j = w_j - \alpha(h_w(x^i)-y^i)x_{j}^{i} $ 

批量梯度下降更新公式：

$w_j = w_j - \alpha \sum_{i=1}^{M}(h_w(x^i)-y^i)x_{j}^{i}$ 

随机梯度下降和批量梯度下降的方法对于原始 PLA 的优化在于收敛速度快，迭代次数较少就可以找到拟合较好的权重向量。

### Lab4——逻辑回归（LR）  

针对一些二分类训练集不是线性可分的，对二分类采用逻辑回归进行优化。采用Sigmoid作为预测函数h，该函数的特性为：自变量大于 0 的时候收敛到 1，小于 0 的时候收敛到0，换句话说就是自变量越大，是 1 的概率也就越大。

考虑整个文本集，根据贝叶斯法则得到最大似然函数：

$ likelihood = \prod_{i=1}^{M}h(x_i)^{y_i}(1-h(x_i))^{1-y_i} $ 

根据最大似然估计，找到 likelihood 的最大值便表示对该 w 对样本划分最好，求likelihood 的最大值，等价于对 likelihood 取对数求最大值，也等价于取对数再取负数求最小值，本实验采取最后一种方法，得到函数如下：

$ Cost(h(x),y) = -\sum_{i=1}^{M}y_ilog(h(x_i)) + (1-y_i)log(1-h(x_i))$  

对Cost函数求偏导，即得到$w_j$下的下降方向，设置下降步长，对$w_j$进行更新，不断地迭代使整体逼近最优解。即权重更新公式为：

$w_j = w_j - \alpha \sum_{i=1}^{M}(h_w(x^i)-y^i)x_{j}^{i}​$

上面的权重更新使用到了整个样本，所以也称为批量梯度下降。如果只考虑单个文本进行更新得到更新公式为（也称随机梯度下降）：

$w_j = w_j - \alpha(h_w(x^i)-y^i)x_{j}^{i}$

关于**学习率**的设置：一般通过手动调节α的大小得到迭代解。当α设置比较大的时候，一方面可以快速下降，不过可能会下降过多而导致越过最优解；当α设置较小的时候，虽然很难越过最优解，但同时也会会导致收敛时间过长。所以在调节α需要选择较为合适的α才能逼近最优解。
**动态学习率** ：即动态调整学习率。当梯度下降降到最优解附近的时候，将学习率降低，避免越过最优解。在这次实验中，当越过最优解下一次步长减半，恢复上一次的权重向量 w 动态调整学习步长。

##Lab5——决策树（ID3，C4.5，CART）

决策树是一种类似于流程图的树结构。一般包括根节点，内部节点以及叶子节点。叶子节点对应于决策结果，其他每个节点对应每个属性下的测试，节点下的分支对应测试的输出。测试样本序列从上往下对应减少上层节点的属性，比如根节点包括了全部属性样本集合，根节点的下一层的每一个子节点的测试序列将不会存在根节点的属性。当决策树建好之后，根节点到每个叶子节点对应了一个判定测试序列，当输入未给定结果的测试样本，会沿着决策树的判定测试序列找到叶子结点，并且返回测试结果。

引入信息熵概念，并对ID3，C4.5，GINI指标进行了分析：

####ID3

ID3通过数据集的信息熵减去在A发生的情况下的条件熵得到信息增益，取信息增益最大的特征作为决策点。

数据集D经验熵：

$$H(D) = −∑_{d∈D}p(d) \ log \ p(d)$$

特征A对数据集D的条件熵：

$$H(D|A) =∑_{a∈A} p(a)H(D|A = a)$$

信息增益：

$$g(D,A) = H(D) − H(D|A)$$

####C4.5

对于 ID3 算法来说，当某一个特征样本值多的时候，会导致极端深度为 2 的树的出现，举个例子：对
训练样本编号（1~N），并把编号也作为一个分类依据的话，那么编号这个属性就会产生 N 个子节点，每个子节点对应了该编号样本的结果，这些子节点的纯度已经是最大的了（条件熵为 0），也就是信息增益
最大，那么决策树就会选择该属性作为根节点，并且划分出 N 个叶子节点，显然这样的决策树不具备泛
化能力。ID3 偏好选择多子节点的特征属性，为了减少这种偏好带来的不利影响，C4.5 决策树将每个特征
条件下的信息增益除以该特征的熵，对于某个特征的子节点越多的时候，该特征的熵也会也会越大，改
善了 ID3 对于可取值数目多的偏好带来的可能不利影响。所以根据信息增益的比例（增益率）作为决策
树分裂依据。

用分裂信息度量来考虑某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息称为属性的内在信息。信息增益率用信息增益 / 内在信息，会导致属性的重要性随着内在信息的增大而减小（如果这个属性本身不确定性就很大，那我就越不倾向于选取它）。这样算是对单纯信息增益有所补偿。

数据集 D 关于特征 A 的值的熵：

$$SplitInfo(D,A) = −∑_{j=1}\frac{|D_j |}{|D|}log(\frac{|D_j |}{|D|} )$$

信息增益率：

$$gRatio(D,A) =\frac{g(D,A)}{SplitInfo(D,A)}$$

#### CART

cart 决策树与 GINI 系数有关。基尼系数的就是熵 (和信息熵差 ln2 倍) 在$ p_i = 1$ 时的一阶泰勒展开

$$plnp = pln(1 + p − 1) ∼ p(p − 1)$$

根据信息熵的定义，熵的值越大表示样本纯度越低。所以选择 gini 系数最小的特征作为决策点。

特征 A 的条件下，数据集 D 的 Gini 系数：

$$Gini(D,A) =∑_{j=1}^{v}p(A_j ) \cdot gini(D_j |A = A_j )$$

其中：$$ gini(D_j |A = A_j ) =∑_{i=1}^{n}p_i (1-p_i) = 1-\sum_{i=1}^{n}p_{i}^2 $$

##Lab6——后向反馈神经网络（BPNN） 

这里推荐一个神经网络种类的介绍（虽然依照我现在的水平还难以驾驭）

https://blog.csdn.net/qq_35082030/article/details/73368962

在本次实验中实现三层神经网络（输入层，隐藏层，输出层），由于是回归问题，隐藏层到输出层不使用激活函数。

#### BPNN 

BPNN 包括了前馈神经网络 (前向传递阶段) 以及误差逆传播（反向传递阶段）两个过程。

* 前向传递阶段

  输入层乘上第 i 个连接权重向量输入到隐藏层的第 i 个神经元节点，隐藏层激活之后乘上连接权重
  向量得到预测 y 值。常用的激活函数有 sigmoid，tanh，ReLu 等。

* 后向传递阶段

  通过输出层输出的 y 值计算损失函数，应用 BGD,SGD 等算法求损失函数的最小值，对权重向量进
  行更新。

####激活函数 

激活函数都是非线性函数，对于神经网络来说，可以有效对非线性数据进行建模（在处理分类问题中，如果激活函数是线性的，而且连接函数也是线性的，就是说神经网络中只存在着线性运算的时候，那么该网络的结果也只是一个线性映射）。由于这三个函数是非线性的，他们都具有非线性映射的学习能力。

* Sigmoid

  $$f(x) =\frac{1}{1 + e^{-x} }$$

  缺点：对于 [−5,5] 之间才有比较好的激活性，当在这个范围外的时候，两侧的梯度比较平缓而且趋近 0（也叫正负饱和区）。而且当它用做中间的隐层的激活层的时候，由于求导会向上一层传递 f(x) ′ ，（f(x)(1− f(x))）。通过观测 Sigmoid 的函数曲线，一旦 x 落入了左右饱和区域，那么 f(x) ′ 接近于 0，所以向上一层传递的梯度也非常小，链式求导法则会使得上一层的梯度也非常小，就会使得网络权重向量难以得到有效训练（梯度消失现象）。

* Tanh

  $$f(x) =\frac{e^x − e^{−x}}{e^x + e^{−x}}$$

  优点：梯度是 Sigmoid 函数梯度的两倍，所以收敛速度会比 Sigmoid 要快。
  缺点：Tanh 通过 sigmoid 的线性变化得到，所以不能改善函数两端过于平坦的现象，依旧对大值敏感度不高，具有 Sigmoid 一样的缺点，也会产生梯度消失的现象。

* ReLU

  $$y = x \ \ x > 0, \ \ \ \ \ \ \ y = 0 \ \ x ≤ 0$$

  优点：ReLU 是对上面两个非线性函数的在右饱和区域的改善。由于 x > 0 的时候导数为 1，所以 ReLU 可以在 x > 0 可以缓解梯度消失的问题。
  缺点：ReLU 有一个很明显的缺点，当 x < 0 的时候，梯度为 0，那么就会导致所有权重不更新（神经元死亡）。对于 ReLU 的改善有 Leaky ReLU，在 x < 0 的时候 y = ax, 可以消除神经元死亡的现象。

#### 梯度消失和梯度爆炸

梯度消失主要是针对多层的神经网络而言的，对于后向传播求梯度，根据链式法则，下一层的梯度
等于上一层的梯度乘上上一层的输入值对权重的求导。就会有多个梯度之间的连乘，当梯度是一个
0 到 1 之间的数的时候，每次后向传到第一层的时候，可能第一层的梯度已经近乎 0 了，导致了权
重向量的变化值极小，这就是梯度消失。对于梯度爆炸就是梯度消失的一个反例，当梯度都很大的
时候，连乘后第一层的梯度极大，导致了权重向量对输入的值很敏感，泛化能力弱，模型不合理，
这就是梯度爆炸带来的不利影响。

##project——多分类预测

基于上面的算法验证了一下效果,提出了一个新的模型。

数据集是从某网站上收集的文本数据集，由于数据集过大，所以在第一步需要对数据进行清洗处理。删去部分没有意义的字母，乱码，对相同含义的单词做一个简单的归类。这一部分需要自己去观察数据并对文本进行分析，不能理所当然地套用处理模板。

在本项目中提出了一种针对数据集文本过大的一种简单的模型。

该模型有以下假设和前提：

1. 某些单词是有特定情感色彩的。比如 delicious 一般出现在好评中，而 just则一般出现在中差评中。因此这些单词在不同分类的文本中，出现的概率一般不会是均匀的


2. 每个单词关于类别的分布是独立的，不会受到上下文的影响
3. 每个单词关于类别的分布是固定的，忽略不同的顾客对餐馆的评价标准的偏差
4. 忽略否定词的影响

模型在数据清洗的基础上，主要由训练词频矩阵、预测句子分类和预测文本分类组成。首先，需要对经过清洗的文本数据进行词汇层面上的统计，归纳出每个单词对在不同类型文本中出现的概率，即分类概率向量。然后根据词汇的分类概率向量，计算出句子的分类概率向量。最后再由句子的分类概率向量投票决定
文本的概率分布向量，进一步确定文本的分类。

这里简单介绍一下如何去训练词频矩阵。详细模型请查看 实验报告\project_MultiClassification.pdf

#### 训练词频矩阵

简单统计每个单词关于各类别的频数，得到一个频数矩阵D（dictionary）。D 的行数是词汇表的大小，列数为 3，分别对应 LOW, MID, HIG。D wd,LOW 表示包含单词 wd 且分类标签为 LOW 的句子中，wd 的总个数为D wd,LOW 。对每一行进行归一化，就得到了概率分布矩阵。但是，由于每个类别的文本长度不同，如中差评的评论字数一般比好评的多，这样会导致好评的概率分布偏低。为了修正这种偏差，在行归一化之前，先对每一列进行归一化。这样我们就初步得到了词频矩阵。







